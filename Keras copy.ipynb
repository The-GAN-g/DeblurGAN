{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "# import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "\n",
    "    return images\n",
    "\n",
    "def get_transform():\n",
    "    transform_list = []\n",
    "    transform_list += [transforms.Resize([360, 640], Image.BICUBIC),\n",
    "#         transforms.Resize([256,256]),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "class CreateDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(CreateDataset, self).__init__()\n",
    "        \n",
    "        dataroot = 'images/'\n",
    "        phase='train/' # train for training and test for testing\n",
    "        self.dir_A = os.path.join(dataroot, phase , 'A') # directory for blur images\n",
    "        self.dir_B = os.path.join(dataroot, phase , 'B') # directory for sharp images\n",
    "        self.A_paths = make_dataset(self.dir_A) # get paths of all blurred images \n",
    "        self.B_paths = make_dataset(self.dir_B) # get paths of all blurred images\n",
    "        self.A_paths = sorted(self.A_paths) # sort the images because all the related images are together\n",
    "        self.B_paths = sorted(self.B_paths)\n",
    "        self.A_size = len(self.A_paths) # 2103 images in A and B each\n",
    "        self.B_size = len(self.B_paths)\n",
    "        self.transform = get_transform() # apply transforms\n",
    "#         print(self.transform)\n",
    "        \n",
    "    def name(self):\n",
    "        return 'BaseDataset'\n",
    "    \n",
    "    def __getitem__(self, index): # not used\n",
    "        A_path = self.A_paths[index % self.A_size]\n",
    "        index_A = index % self.A_size\n",
    "        B_path = self.B_paths[index % self.A_size]\n",
    "        index_B = index % self.B_size\n",
    "#        print('(A, B) = (%d, %d)' % (index_A, index_B))\n",
    "        A_img = Image.open(A_path).convert('RGB')\n",
    "        B_img = Image.open(B_path).convert('RGB')\n",
    "        \n",
    "        A_img = self.transform(A_img)\n",
    "        B_img = self.transform(B_img)\n",
    "        return {'A': A_img, 'B': B_img,\n",
    "                'A_paths': A_path, 'B_paths': B_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(self.A_size, self.B_size)\n",
    "\n",
    "class CreateDataLoader():\n",
    "    def name(self):\n",
    "        return 'CreateDataLoader'\n",
    "\n",
    "    def __init__(self, batchSize):\n",
    "        super(CreateDataLoader,self).__init__()\n",
    "#         batchSize = 1\n",
    "        self.dataset = CreateDataset() # Call to create dataset class\n",
    "        self.dataloader = data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size = batchSize,\n",
    "            shuffle = False\n",
    "        )\n",
    "\n",
    "    def load_data(self):\n",
    "        return self.dataloader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not to run\n",
    "# to check if data is loaded\n",
    "\n",
    "# data_loader = CreateDataLoader()\n",
    "# dataset = data_loader.load_data()\n",
    "\n",
    "# def imshow(img):\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "\n",
    "# for i, data in enumerate(dataset):\n",
    "# # show images\n",
    "#     if i==0:\n",
    "#         fig = plt.figure(figsize=(20, 15))\n",
    "#         imshow(torchvision.utils.make_grid(data['A']))\n",
    "#         imshow(torchvision.utils.make_grid(data['B']))\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):   \n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "    def __init__(self, input_nc = 3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False,\n",
    "                 use_parallel=True):  \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "         Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images. For color images this is 3.\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer (can be nn.BatchNorm2d or nn.InstanceNorm2d)\n",
    "        \"\"\"\n",
    "        \n",
    "#         ndf = 64\n",
    "#         output_nc = 3\n",
    "#         input_shape_discriminator = (256, 256, output_nc)    \n",
    "#         n_layers = 3\n",
    "#         use_sigmoid = False\n",
    "#         gpu_ids = []\n",
    "#         norm_layer = nn.BatchNorm2d\n",
    "#         ndf = 64\n",
    "#         input_nc = 3\n",
    "#         use_parallel = True\n",
    "\n",
    "        kw = 4 # kernel size\n",
    "        padw = int(np.ceil((kw - 1) / 2)) # 2\n",
    "\n",
    "        sequence = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        \n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers): # runs for 2 iterations\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8) # n_layers = 3\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                          kernel_size=kw, stride=2, padding=padw), # default bias value is True\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "        # output image till here is 32 * 32 * 256\n",
    "        \n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8) \n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        \n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "        \n",
    "    def forward(self, input):\n",
    "#         if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor) and self.use_parallel:\n",
    "#             return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "#         else:\n",
    "            return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Discriminator()\n",
    "model\n",
    "\n",
    "# pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "# pytorch_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):   \n",
    "    \"\"\"Defines a RESNET Generator\"\"\"\n",
    "    def __init__(self, input_nc = 3, ngf = 64, n_layers=3, norm_layer=nn.BatchNorm2d, use_dropout=True, \n",
    "                 n_blocks = 9, learn_residual = True,\n",
    "                 use_parallel=True, padding_type='reflect'):\n",
    "        assert (n_blocks >= 0)\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "#         use_gpu = len(gpu_ids) > 0\n",
    "        \n",
    "#         if use_gpu:\n",
    "#             assert (torch.cuda.is_available())\n",
    "        \n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n",
    "            norm_layer(ngf),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        \n",
    "        n_downsampling = 2\n",
    "        \n",
    "        # Increase filter number\n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            norm_layer(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            norm_layer(256),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        \n",
    "        # Apply 9 ResNet blocks\n",
    "        for i in range(n_blocks):\n",
    "            model += [\n",
    "                ResnetBlock(256, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "            ]\n",
    "          \n",
    "        # Decrease filter number to 3 (RGB)\n",
    "        model += [\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            norm_layer(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            norm_layer(64),\n",
    "            nn.ReLU(True),\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, out_channels = 3 , kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, input):\n",
    "#         if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor) and self.use_parallel:\n",
    "#             output = nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "#         else:\n",
    "        output = self.model(input)\n",
    "        \n",
    "        # Add direct connection from input to output and re-center to [-1, 1]\n",
    "#         if self.learn_residual:\n",
    "            # output = input + output\n",
    "#             https://pytorch.org/docs/stable/torch.html#torch.clamp\n",
    "        output = torch.clamp(input + output, min=-1, max=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"RESNET Block\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    Instantiate a PyTorch Resnet Block using Sequential API.\n",
    "        input: Input tensor\n",
    "        filters: Number of filters to use\n",
    "        kernel_size: Shape of the kernel for the convolution\n",
    "        strides: Shape of the strides for the convolution\n",
    "        use_dropout: Boolean value to determine the use of dropout\n",
    "        return: Pytorch Model\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        \n",
    "        blocks = [ \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3),\n",
    "            norm_layer(dim),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        \n",
    "        if use_dropout:\n",
    "            blocks += [\n",
    "                nn.Dropout(0.5)\n",
    "            ]\n",
    "        \n",
    "        blocks += [\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3),\n",
    "            norm_layer(dim)\n",
    "        ]\n",
    "            \n",
    "        self.conv_block = nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Two convolution layers followed by a direct connection between input and output\n",
    "            out = x + self.conv_block(x)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (23): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (27): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator()\n",
    "model\n",
    "# pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "# pytorch_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class PerceptualLoss():\n",
    "    def contentFunc(self):\n",
    "        conv_3_3_layer = 14\n",
    "        cnn = models.vgg19(pretrained=True).features\n",
    "        cnn = cnn.cuda()\n",
    "        model = nn.Sequential()\n",
    "        model = model.cuda()\n",
    "        for i, layer in enumerate(list(cnn)):\n",
    "            model.add_module(str(i), layer)\n",
    "            if i == conv_3_3_layer:\n",
    "                break\n",
    "        return model\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        self.criterion = loss\n",
    "        self.contentFunc = self.contentFunc()\n",
    "\n",
    "    def get_loss(self, fakeIm, realIm):\n",
    "        f_fake = self.contentFunc.forward(fakeIm)\n",
    "        f_real = self.contentFunc.forward(realIm)\n",
    "        f_real_no_grad = f_real.detach()\n",
    "        loss = self.criterion(f_fake, f_real_no_grad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loss = PerceptualLoss(nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PerceptualLoss at 0x215b357e400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DiscLossWGANGP(DiscLossLS):\n",
    "#     def name(self):\n",
    "#         return 'DiscLossWGAN-GP'\n",
    "\n",
    "#     def __init__(self, opt, tensor):\n",
    "#         super(DiscLossWGANGP, self).__init__(opt, tensor)\n",
    "#         # DiscLossLS.initialize(self, opt, tensor)\n",
    "#         self.LAMBDA = 10\n",
    "\n",
    "#     def get_g_loss(self, net, realA, fakeB):\n",
    "#         # First, G(A) should fake the discriminator\n",
    "#         self.D_fake = net.forward(fakeB)\n",
    "#         return -self.D_fake.mean()\n",
    "\n",
    "#     def calc_gradient_penalty(self, netD, real_data, fake_data):\n",
    "#         alpha = torch.rand(1, 1)\n",
    "#         alpha = alpha.expand(real_data.size())\n",
    "#         alpha = alpha.cuda()\n",
    "\n",
    "#         interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "#         interpolates = interpolates.cuda()\n",
    "#         interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "#         disc_interpolates = netD.forward(interpolates)\n",
    "\n",
    "#         gradients = autograd.grad(\n",
    "#             outputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "#             create_graph=True, retain_graph=True, only_inputs=True\n",
    "#         )[0]\n",
    "\n",
    "#         gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.LAMBDA\n",
    "#         return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from dhruva\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "def init_loss(tensor):\n",
    "    # disc_loss = None\n",
    "    # content_loss = None\n",
    "    \n",
    "    content_loss = PerceptualLoss(nn.MSELoss())\n",
    "    # content_loss.initialize(nn.MSELoss())\n",
    "\n",
    "    disc_loss = DiscLoss(tensor)\n",
    "\n",
    "    return disc_loss, content_loss\n",
    "\n",
    "def real_mse_loss(y_true, y_pred):\n",
    "    # how close is the produced output from being \"real\"?\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(y_pred,y_true)\n",
    "    return loss\n",
    "    #return torch.mean((D_out-1)**2)\n",
    "\n",
    "def fake_mse_loss(y_true, y_pred):\n",
    "    # how close is the produced output from being \"fake\"?\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(y_pred,y_true)\n",
    "    return loss\n",
    "    #return torch.mean(D_out**2)\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    cnn = models.vgg19(pretrained=True).features\n",
    "    cnn = cnn.cuda()\n",
    "    model = nn.Sequential()\n",
    "    model = model.cuda()\n",
    "    for i,layer in enumerate(list(cnn)):\n",
    "        model.add_module(str(i),layer)\n",
    "        if i == 'conv_3_3_layer':\n",
    "            break\n",
    "    return torch.mean((y_true-y_pred)**2)\n",
    "\n",
    "class PerceptualLoss():\n",
    "    def contentFunc(self):\n",
    "        conv_3_3_layer = 14\n",
    "        cnn = models.vgg19(pretrained=True).features\n",
    "        cnn = cnn.cuda()\n",
    "        model = nn.Sequential()\n",
    "        model = model.cuda()\n",
    "        for i, layer in enumerate(list(cnn)):\n",
    "            model.add_module(str(i), layer)\n",
    "            if i == conv_3_3_layer:\n",
    "                break\n",
    "        return model\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        self.criterion = loss\n",
    "        self.contentFunc = self.contentFunc()\n",
    "\n",
    "    def get_loss(self, fakeIm, realIm):\n",
    "        f_fake = self.contentFunc.forward(fakeIm)\n",
    "        f_real = self.contentFunc.forward(realIm)\n",
    "        f_real_no_grad = f_real.detach()\n",
    "        loss = self.criterion(f_fake, f_real_no_grad)\n",
    "        return loss\n",
    "    \n",
    "class DiscLoss:\n",
    "    def name(self):\n",
    "        return 'DiscLoss'\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        \n",
    "\n",
    "    def get_g_loss(self,net, realA, fakeB):\n",
    "        # First, G(A) should fake the discriminator\n",
    "        pred_fake = net.forward(fakeB)\n",
    "        return self.criterion(pred_fake, 1)\n",
    "\n",
    "    def get_loss(self, net, realA, fakeB, realB):\n",
    "        # Fake\n",
    "        # stop backprop to the generator by detaching fake_B\n",
    "        # Generated Image Disc Output should be close to zero\n",
    "        self.pred_fake = net.forward(fakeB.detach())\n",
    "        self.loss_D_fake = self.criterionGAN(self.pred_fake, 0)\n",
    "\n",
    "        # Real\n",
    "        self.pred_real = net.forward(realB)\n",
    "        self.loss_D_real = self.criterionGAN(self.pred_real, 1)\n",
    "\n",
    "        # Combined loss\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        return self.loss_D\n",
    "    \n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return torch.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "#Gen_Dis(G,D)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    G.to(device)\n",
    "    D.to(device)\n",
    " #   G_D.to(device)\n",
    "    print('Models moved to GPU.')\n",
    "else:\n",
    "    print('Only CPU available.')\n",
    "\n",
    "        \n",
    "# hyperparams for Adam optimizer\n",
    "lr=1E-4\n",
    "beta1=0.9\n",
    "beta2=0.999 # default value\n",
    "\n",
    "# Create optimizers for the generators and discriminators\n",
    "optimizer_G = torch.optim.Adam( G.parameters(), lr=lr, betas=(beta1, beta2) )\n",
    "optimizer_D = torch.optim.Adam( D.parameters(), lr=lr, betas=(beta1, beta2) )\n",
    "#optimizer_G_D = torch.optim.Adam( G_D.parameters(), lr=lr, betas=(beta1, beta2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor as Tensor\n",
    "\n",
    "def training_loop(n_epochs=100):\n",
    "    batch_size = 1\n",
    "    data_loader=CreateDataLoader(batch_size)\n",
    "    dataset = data_loader.load_data()\n",
    "    print_every=10\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "#     batch_size = 1\n",
    "    # keep track of losses over time\n",
    "    losses = []\n",
    "    \n",
    "\n",
    "    # the \"_\" is a placeholder for no labels\n",
    "\n",
    "    # batches per epoch\n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        for i, data in enumerate(dataset):\n",
    "            D.train()\n",
    "            G.train()\n",
    "            \n",
    "            images_X=data['A']\n",
    "            images_Y=data['B']\n",
    "   \n",
    "            # move images to GPU if available (otherwise stay on CPU)\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            real_X = images_X.to(device)\n",
    "            real_Y = images_Y.to(device)\n",
    "\n",
    "\n",
    "            # ============================================\n",
    "            #            TRAIN THE DISCRIMINATORS\n",
    "            # ============================================\n",
    "            \n",
    "            \n",
    "            # 2. Generate fake images that look like domain X based on real images in domain Y\n",
    "            \n",
    "            fake_Y = G(real_X)\n",
    "            \n",
    "            ##   First: D_X, real and fake loss components   ##\n",
    "\n",
    "            # Train with real images\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # 1. Compute the discriminator losses on real images\n",
    "            out_x = D(real_Y)\n",
    "            output_true_batch, output_false_batch = Tensor(out_x.size()).fill_(1), Tensor(out_x.size()).fill_(-1)\n",
    "            output_true_batch = output_true_batch.to(device)\n",
    "            output_false_batch = output_false_batch.to(device)\n",
    "        \n",
    "            D_X_real_loss = real_mse_loss(out_x,output_true_batch)\n",
    "\n",
    "            # Train with fake images\n",
    "\n",
    "            \n",
    "            # 3. Compute the fake loss for D_X\n",
    "            out_x = D(fake_Y)\n",
    "        \n",
    "            D_X_fake_loss = fake_mse_loss(out_x,output_false_batch)\n",
    "\n",
    "\n",
    "            # 4. Compute the total loss and perform backprop\n",
    "            d_x_loss = (D_X_real_loss+D_X_fake_loss)/2\n",
    "            d_x_loss.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "\n",
    "\n",
    "\n",
    "            # =========================================\n",
    "            #            TRAIN THE GENERATORS\n",
    "            # =========================================\n",
    "\n",
    "            ##    First: generate fake X images and reconstructed Y images    ##\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # 1. Generate fake images that look like domain X based on real images in domain Y\n",
    "           # fake_X = G(images_X)\n",
    "\n",
    "            # 2. Compute the generator loss based on domain X\n",
    "            out_x = D(fake_Y)\n",
    "     \n",
    "            g_loss = real_mse_loss(out_x,output_true_batch)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "\n",
    "            # Print the log info\n",
    "            if epoch % print_every == 0:\n",
    "                # append real and fake discriminator losses and the generator loss\n",
    "                losses.append((d_x_loss.item(), g_loss.item()))\n",
    "                print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f}  | g_total_loss: {:6.4f}'.format(\n",
    "                        epoch, n_epochs, d_x_loss.item(), g_loss.item()))\n",
    "\n",
    "\n",
    "            sample_every=10000\n",
    "            # Save the generated samples\n",
    "            if epoch % sample_every == 0:\n",
    "                G.eval() # eval mode for generating samples\n",
    "                save_samples(epoch, fixed_Y, fixed_X, G , batch_size=2)\n",
    "                G.train()\n",
    "\n",
    "            if d_x_loss <= valid_loss_min:\n",
    "                torch.save(G.state_dict(), 'model_G.pt')\n",
    "                torch.save(D.state_dict(), 'model_D.pt')\n",
    "                valid_loss_min = d_x_loss\n",
    "\n",
    "            # uncomment these lines, if you want to save your model\n",
    "            checkpoint_every=1000\n",
    "            # Save the model parameters\n",
    "            if epoch % checkpoint_every == 0:\n",
    "                checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-25c72f3fa56f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-8a36dc99e1aa>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0md_x_loss\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mvalid_loss_min\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model_G.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model_D.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = training_loop(n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
